{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morhunenko Mykola. Fourth variant. \"Peninsula\" region\n",
    "`descriptive` file was taken as a template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pyflux as pf\n",
    "import statsmodels as ss\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.api import ExponentialSmoothing\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.tsa.api as smt\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from fbprophet import Prophet\n",
    "from xgboost import XGBRegressor\n",
    "from pandas import to_datetime\n",
    "from fbprophet.diagnostics import cross_validation\n",
    "from fbprophet.diagnostics import performance_metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, TimeDistributed, Conv1D, MaxPooling1D, Flatten, RepeatVector\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam, Adagrad\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe, rand\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.dpi']= 100\n",
    "matplotlib.rcParams['figure.figsize'] = 15, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGIONS = [['JH', 'Johor', 'JH'], \n",
    "           ['PH', 'Pahang', 'PH'],\n",
    "           ['PRK', 'Perak', 'PRK'], \n",
    "           ['OtherPEN', 'Other Pen. States', 'OtherPEN'],\n",
    "           ['PEN', 'Peninsula', 'Pmalay'],\n",
    "           ['SBH', 'Sabah', 'SBH'],\n",
    "           ['SWK', 'Sarawak', 'SWK']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "production = pd.read_csv('palm_data/production_good.csv')\n",
    "rainfall = pd.read_csv('palm_data/rainfall_good.csv')\n",
    "area = pd.read_csv('palm_data/area_good.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAINFALL_LAGS = [6, 7, 8, 9, 10, 11, 12]\n",
    "PRODUCTION_LAGS = [6, 7, 8, 9, 10, 11, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(production, rainfall, area, REGIONS, PRODUCTION_LAGS, RAINFALL_LAGS):\n",
    "    output = {}\n",
    "    for i, region in enumerate(REGIONS):\n",
    "        data = production[production.Region == region[0]]\n",
    "        data = pd.merge(data[['Year', 'Month', 'Production', 'Diff_production']],\n",
    "                        rainfall[rainfall.Region == region[1]][['Year', 'Month', 'Rainfall']], \n",
    "                        on=['Year', 'Month'], how='left')\n",
    "\n",
    "        data = pd.merge(data, area[area.Region == region[2]][\n",
    "            ['Year', 'Area_ma', 'Area_npa', 'Area_rpa', 'Area_New', 'Area_ma_new']], on='Year', how='left')\n",
    "            \n",
    "        data['Year'] = data['Year'].astype(int)\n",
    "        data['Month'] = data['Month'].astype(int)\n",
    "        data.reset_index(inplace=True)\n",
    "\n",
    "        data.set_index([pd.to_datetime(['{0}-{1}-01'.format(x, y) for (x, y) in zip(data.Year, data.Month)])],\n",
    "                       inplace=True)\n",
    "\n",
    "        data.drop(['index', 'Year', 'Area_New', 'Diff_production'], axis=1, inplace=True)\n",
    "        data['Time'] = np.arange(len(data))\n",
    "\n",
    "        for lag in RAINFALL_LAGS:\n",
    "            temp = np.concatenate((np.array([np.nan for _ in range(lag)]), data.Rainfall.values[:-lag]))\n",
    "            data['Rainfall_{0}'.format(lag)] = temp\n",
    "\n",
    "        for lag in PRODUCTION_LAGS:\n",
    "            temp = np.concatenate((np.array([np.nan for _ in range(lag)]), data.Production.values[:-lag]))\n",
    "            data['Production_{0}'.format(lag)] = temp\n",
    "        \n",
    "        PRODUCTION = data.Production\n",
    "        data.drop(['Production'], axis=1, inplace=True)\n",
    "\n",
    "        data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "        #  And finally drop rainfalls\n",
    "        data.drop(['Rainfall'], axis=1, inplace=True)\n",
    "        ### HERE I CAN ADD FEATURE ENGINEERING!!!\n",
    "\n",
    "        #  And clip first year\n",
    "        for col in data.columns:\n",
    "            data['_'.join([region[1], str(col)])] = data[col]\n",
    "            data.drop([col], axis=1, inplace=True)\n",
    "        \n",
    "        output[region[1]] = (data[max(PRODUCTION_LAGS):], PRODUCTION[max(PRODUCTION_LAGS):])\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = process_data(production, rainfall, area, REGIONS, PRODUCTION_LAGS, RAINFALL_LAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "DATA.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA=DATA['Peninsula']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(len(DATA[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stationarity\n",
    "Time-series property that indicates that the chosen Time Series has constant mean, variance, autocorrelation structure and have no periodic components over time. To get a stationary time series, we can split it into components (trend, seasonality and residuals) and take that residual. Thay can be stationary, but to be sure, we have to test it (for example, use Augmented Dickey-Fuller test).\n",
    "#### Differencing\n",
    "One more method to make a series stationary. If we have lagged data and can see autocorrelations in it, we can subtract from our series the same series, but lagged by one. More simple: from every element $E_{t}$ subtract $E_{t-1}$\n",
    "#### Moving Average, Exponential Smoothing\n",
    "Both - techniques to extract useful patterns from the series. \n",
    "Moving average - we chose a window and simply slide over the series with that window and calculate average. Can be used to find a trend, seasonality and a little bit of noise.\n",
    "Exponential Smoothing - when our weights in the observation history are different. For example, nearest data have a bigger impact than older"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additive VS multiplicative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA[1].plot(figsize=(15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a time series, that can be either additive ot multiplicative. This cpecific looks like additive, but the task is to make all on both data. Firstle, let's split the series on the trend, seasonality and residuals. As far as we have monthly data, period have to be 12. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "ss_decomposition = seasonal_decompose(x=DATA[1], model='additive', period=12)\n",
    "estimated_trend_add = ss_decomposition.trend\n",
    "estimated_seasonal_add = ss_decomposition.seasonal\n",
    "estimated_residual_add = ss_decomposition.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 1, sharex=True, sharey=False)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "axes[0].plot(DATA[1], label='Original')\n",
    "axes[0].legend(loc='upper left');\n",
    "\n",
    "axes[1].plot(estimated_trend_add, label='Trend')\n",
    "axes[1].legend(loc='upper left');\n",
    "\n",
    "axes[2].plot(estimated_seasonal_add, label='Seasonality')\n",
    "axes[2].legend(loc='upper left');\n",
    "\n",
    "axes[3].plot(estimated_residual_add, label='Residuals')\n",
    "axes[3].legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_residual_add = estimated_residual_add.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_residual_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.Series(estimated_residual_add).hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplicative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "ss_decomposition = seasonal_decompose(x=DATA[1], model='multiplicative', period=12)\n",
    "estimated_trend_mul = ss_decomposition.trend\n",
    "estimated_seasonal_mul = ss_decomposition.seasonal\n",
    "estimated_residual_mul = ss_decomposition.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 1, sharex=True, sharey=False)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "axes[0].plot(DATA[1], label='Original')\n",
    "axes[0].legend(loc='upper left');\n",
    "\n",
    "axes[1].plot(estimated_trend_mul, label='Trend')\n",
    "axes[1].legend(loc='upper left');\n",
    "\n",
    "axes[2].plot(estimated_seasonal_mul, label='Seasonality')\n",
    "axes[2].legend(loc='upper left');\n",
    "\n",
    "axes[3].plot(estimated_residual_mul, label='Residuals')\n",
    "axes[3].legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_residual_mul = estimated_residual_mul.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_residual_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(estimated_residual_mul).hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_add, pvalue_add, usedlag_add, nobs_add, critical_values_add, icbest_add = adfuller(estimated_residual_add)\n",
    "adf_mul, pvalue_mul, usedlag_mul, nobs_mul, critical_values_mul, icbest_mul = adfuller(estimated_residual_mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adf_add, adf_mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pvalue_add, pvalue_mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(critical_values_add, \"\\n\",critical_values_mul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using that p-values, we can decide, which model is better to choose, but let's continue with both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove autocorelation with differencing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more way to get stationary series is to aplly difference method. it can be applied to a lagged data, and here we have such data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = DATA[1] - DATA[1].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_residual_dif = pd.Series(difference, index=DATA[1][1:].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.arange(len(difference)), difference)\n",
    "estimated_residual_dif.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_dif, pvalue_dif, usedlag_dif, nobs_dif, critical_values_dif, icbest_dif = adfuller(estimated_residual_dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adf_dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pvalue_dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(critical_values_dif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now if we compare pvalues of decomposition and differencing methods, it is better to take the assumption that the series is additive and use decomposition. but for now we have three series: estimated_residual_add, estimated_residual_dif, estimated_residual_mul and will work with all of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving Average, Exponential Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mooving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = [estimated_residual_add, estimated_residual_mul, estimated_residual_dif]\n",
    "\n",
    "ts_moving_avg_right = DATA[1].rolling(12, center=False).mean()\n",
    "plt.plot(ts_moving_avg_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get our stationary series, residuals, using MA result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_ma_diff = DATA[1] - ts_moving_avg_right\n",
    "ts_ma_diff.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET.append(ts_ma_diff.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationarity_test(ts):\n",
    "    # only now I found the stationarity test in the template file, so I will use it\n",
    "    \n",
    "    rolmean = ts.rolling(12).mean()\n",
    "    rolstd = ts.rolling(12).std()\n",
    "\n",
    "    orig = plt.plot(ts, color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "\n",
    "    # Dickey-Fuller test:\n",
    "    print(\"Results of Dickey-Fuller Test:\")\n",
    "    dftest = adfuller(ts, autolag='AIC')\n",
    "    print(dftest)     \n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_test(DATASET[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have one more candidate to be our stationary series according to the test (but easy to see seasonality there..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expwighted_avg = DATA[1].ewm(span=12).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_test(expwighted_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA[1].plot(color=\"blue\", label=\"Original\")\n",
    "expwighted_avg.plot(color=\"red\", label=\"EWMA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_exp_diff = DATA[1] - expwighted_avg\n",
    "ts_exp_diff.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_test(ts_exp_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and one more candidate for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET.append(ts_exp_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(DATASET)\n",
    "DATASET = [each.dropna() for each in DATASET]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOW WE HAVE FIVE STATIONARY  SERIES\n",
    "all of them are in the DATASET array. \n",
    "- DATASET[0] - residuals got after splitting input data using decomposition and suppose that it is additive\n",
    "- DATASET[1] - residuals got after splitting input data using decomposition and suppose that it is multiplictive\n",
    "- DATASET[2] - series got using differencing\n",
    "- DATASET[3] - series got after substracting all features that was found with MA\n",
    "- DATASET[4] - series got after substracting all features that was found with EWMA\n",
    "# **ALL next plots will be in this order **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Average (just for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for counter in range(len(DATASET)):\n",
    "    train = DATASET[counter][:-112]\n",
    "    test = DATASET[counter][-113:-101]\n",
    "    trend_seasonal_avg = np.mean(DATASET[counter])\n",
    "    time = np.arange(len(DATASET[counter]) - 100)\n",
    "    simple_avg_preds = np.full(shape=12, fill_value=trend_seasonal_avg, dtype='float')\n",
    "    plt.plot(time[:-12], train, 'b--', label=\"train\")\n",
    "    plt.plot(time[-12:], test, color='orange', linestyle=\"--\", label=\"test\")\n",
    "    plt.plot(time[-12:], simple_avg_preds, 'r--', label=\"predictions\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(\"Simple Average Smoothing\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show();\n",
    "    \n",
    "    print(\"MSE = {}\".format(mse(test, simple_avg_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "useless (especcialy in this case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triple Exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for counter in range(len(DATASET)):\n",
    "    i += 1\n",
    "    train = DATASET[counter][:-112]\n",
    "    test = DATASET[counter][-113:-101]\n",
    "    time = np.arange(len(DATASET[counter]) -100)\n",
    "    if (i == 2):\n",
    "        triple = ExponentialSmoothing(train,\n",
    "                              trend=\"multiplicative\",\n",
    "                              seasonal=\"multiplicative\",\n",
    "                              seasonal_periods=12).fit(optimized=True)\n",
    "    else:\n",
    "        triple = ExponentialSmoothing(train,\n",
    "                              trend=\"additive\",\n",
    "                              seasonal=\"additive\",\n",
    "                              seasonal_periods=12).fit(optimized=True)\n",
    "    triple_preds = triple.forecast(len(test))\n",
    "    \n",
    "    plt.plot(time[:-12], train, 'b--', label=\"train\")\n",
    "    plt.plot(time[-12:], test, color='orange', linestyle=\"--\", label=\"test\")\n",
    "    plt.plot(time[-12:], triple_preds, 'r--', label=\"predictions\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(\"Triple Exponential Smoothing\")\n",
    "    plt.grid(alpha=0.3);\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"MSE = {}\".format(mse(test, triple_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that this approach gives us not bad predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AR, MA, ARMA, ARIMA, (SARIMAX MODULE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(data, fit):\n",
    "    return 100*(np.average(abs((fit-data)/data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper plot function for visualization\n",
    "\n",
    "def plots(data, lags=None):\n",
    "    layout = (1, 3)\n",
    "    raw  = plt.subplot2grid(layout, (0, 0))\n",
    "    acf  = plt.subplot2grid(layout, (0, 1))\n",
    "    pacf = plt.subplot2grid(layout, (0, 2))\n",
    "    \n",
    "    data.plot(ax=raw)\n",
    "    smt.graphics.plot_acf(data, lags=lags, ax=acf)\n",
    "    smt.graphics.plot_pacf(data, lags=lags, ax=pacf)\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in DATASET:\n",
    "    plots(each, lags=48);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "according to autocorelation function results, p=3 (4th zeo) <br>\n",
    "according to partial autocorelation function results, q=2 (3d zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dt in DATASET:\n",
    "\n",
    "    # fit SARIMA monthly based on helper plots\n",
    "    sar = sm.tsa.statespace.SARIMAX(dt, \n",
    "                                    order=(3,0,0),  \n",
    "                                    trend='c').fit()\n",
    "\n",
    "    print(sar.plot_diagnostics())\n",
    "    plots(dt, lags=48)\n",
    "    print(sar.summary())\n",
    "    res = sar.predict(start = 0, end= len(dt), dynamic=False)\n",
    "    res.plot()\n",
    "    dt.plot()\n",
    "    \n",
    "    print(\"MSE = {}\".format(mse(dt, res[:-1])))\n",
    "    print(\"MAPE = {}\".format(MAPE(dt, res[:-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dt in DATASET:\n",
    "\n",
    "    # fit SARIMA monthly based on helper plots\n",
    "    sar = sm.tsa.statespace.SARIMAX(dt, \n",
    "                                    order=(0,0,2),  \n",
    "                                    trend='c').fit()\n",
    "\n",
    "    print(sar.plot_diagnostics())\n",
    "    plots(dt, lags=48)\n",
    "    print(sar.summary())\n",
    "    res = sar.predict(start = 0, end= len(dt), dynamic=False)\n",
    "    res.plot()\n",
    "    dt.plot()\n",
    "    \n",
    "    print(\"MSE = {}\".format(mse(dt, res[:-1])))\n",
    "    print(\"MAPE = {}\".format(MAPE(dt, res[:-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dt in DATASET:\n",
    "\n",
    "    # fit SARIMA monthly based on helper plots\n",
    "    sar = sm.tsa.statespace.SARIMAX(dt, \n",
    "                                    order=(3,0,2),  \n",
    "                                    trend='c').fit()\n",
    "\n",
    "    print(sar.plot_diagnostics())\n",
    "    plots(dt, lags=48)\n",
    "    print(sar.summary())\n",
    "    res = sar.predict(start = 0 , end= len(dt), dynamic=False)\n",
    "    res.plot()\n",
    "    dt.plot()\n",
    "    \n",
    "    print(\"MAPE = {}\".format(MAPE(dt, res[:-1])))\n",
    "    print(\"MSE = {}\".format(mse(dt, res[:-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for dt in DATASET:\n",
    "\n",
    "    # fit SARIMA monthly based on helper plots\n",
    "    sar = sm.tsa.statespace.SARIMAX(dt, \n",
    "                                    order=(3,1,2),  \n",
    "                                    trend='c').fit()\n",
    "\n",
    "    print(sar.plot_diagnostics())\n",
    "    plots(dt, lags=48)\n",
    "    print(sar.summary())\n",
    "    res = sar.predict(start = 0 , end= len(dt), dynamic=False)\n",
    "    res.plot()\n",
    "    dt.plot()\n",
    "    \n",
    "    print(\"MSE = {}\".format(mse(dt, res[:-1])))\n",
    "    print(\"MAPE = {}\".format(MAPE(dt, res[:-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ML modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# main source: https://machinelearningmastery.com/xgboost-for-time-series-forecasting/\n",
    "#\n",
    "# doesn`t complete\n",
    "# forecast monthly births with xgboost\n",
    "\n",
    "# from numpy import asarray\n",
    "# from pandas import read_csv\n",
    "# from pandas import DataFrame\n",
    "# from pandas import concat\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# from xgboost import XGBRegressor\n",
    "# from matplotlib import pyplot\n",
    "\n",
    "# # transform a time series dataset into a supervised learning dataset\n",
    "# def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "#     n_vars = 1\n",
    "#     df = DataFrame(data)\n",
    "#     cols = list()\n",
    "#     # input sequence (t-n, ... t-1)\n",
    "#     for i in range(n_in, 0, -1):\n",
    "#         cols.append(df.shift(i))\n",
    "#     # forecast sequence (t, t+1, ... t+n)\n",
    "#     for i in range(0, n_out):\n",
    "#         cols.append(df.shift(-i))\n",
    "#     # put it all together\n",
    "#     agg = concat(cols, axis=1)\n",
    "#     # drop rows with NaN values\n",
    "#     if dropnan:\n",
    "#         agg.dropna(inplace=True)\n",
    "#     return agg.values\n",
    "\n",
    "# # fit an xgboost model and make a one step prediction\n",
    "# def xgboost_forecast(train, testX):\n",
    "#     # transform list into array\n",
    "#     train = asarray(train)\n",
    "#     # split into input and output columns\n",
    "#     trainX, trainy = train[:, :-1], train[:, -1]\n",
    "#     # fit model\n",
    "#     model = XGBRegressor(max_depth=4, objective='reg:squarederror', n_estimators=1000)\n",
    "#     model.fit(trainX, trainy)\n",
    "#     # make a one-step prediction\n",
    "#     yhat = model.predict(asarray([testX]), validate_features=True)\n",
    "#     return yhat[0]\n",
    "\n",
    "# # walk-forward validation for univariate data\n",
    "# # def walk_forward_validation(data, n_test):\n",
    "# #     predictions = list()\n",
    "# #     # split dataset\n",
    "# #     train, test = data[:-12], data[-12:]\n",
    "# #     # seed history with training dataset\n",
    "# #     history = [x for x in train]\n",
    "# #     # step over each time-step in the test set\n",
    "# #     for i in range(len(test)):\n",
    "# #         # split test row into input and output columns\n",
    "# #         testX, testy = test[i, :-1], test[i, -1]\n",
    "# #         # fit model on history and make a prediction\n",
    "# #         yhat = xgboost_forecast(history, testX)\n",
    "# #         # store forecast in list of predictions\n",
    "# #         predictions.append(yhat)\n",
    "# #         # add actual observation to history for the next loop\n",
    "# #         history.append(test[i])\n",
    "# #         # summarize progress\n",
    "# #         print('>expected=%.1f, predicted=%.1f' % (testy, yhat))\n",
    "# #     # estimate prediction error\n",
    "# #     error = mean_absolute_error(test[:, -1], predictions)\n",
    "# #     return error, test[:, -1], predictions\n",
    "\n",
    "# # load the dataset\n",
    "# # series = DATA[1]\n",
    "# # values = series.values\n",
    "# # # transform the time series data into supervised learning\n",
    "# # data = series_to_supervised(values, n_in=6)\n",
    "# # # evaluate\n",
    "# # mae, y, yhat = walk_forward_validation(data, 12)\n",
    "\n",
    "# # print('MAE: %.3f' % mae)\n",
    "# # # plot expected vs preducted\n",
    "# # pyplot.plot(y, label='Expected')\n",
    "# # pyplot.plot(yhat, label='Predicted')\n",
    "# # pyplot.legend()\n",
    "# # pyplot.show()\n",
    "# # #-------------------------------\n",
    "# predictions = list()\n",
    "    \n",
    "# history = [x for x in data]\n",
    "\n",
    "# for i in range(12):\n",
    "#     # split test row into input and output columns\n",
    "#     testX, testy = test[i, :-1], test[i, -1]\n",
    "#     # fit model on history and make a prediction\n",
    "#     yhat = xgboost_forecast(history, testX)\n",
    "#     # store forecast in list of predictions\n",
    "#     predictions.append(yhat)\n",
    "#     # add actual observation to history for the next loop\n",
    "#     history.append(test[i])\n",
    "\n",
    "# values = series.values\n",
    "# # transform the time series data into supervised learning\n",
    "# train = series_to_supervised(values, n_in=6)\n",
    "# # split into input and output columns\n",
    "# trainX, trainy = train[:, :-1], train[:, -1]\n",
    "# # fit model\n",
    "# model = XGBRegressor(objective='reg:squarederror', n_estimators=1000)\n",
    "# model.fit(trainX, trainy)\n",
    "# # construct an input for a new preduction\n",
    "# row = values[-6:].flatten()\n",
    "# # make a one-step prediction\n",
    "# # yhat = model.predict(asarray([row]))\n",
    "# # print('Input: %s, Predicted: %.3f' % (row, yhat[0]))\n",
    "# plt.plot(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainy.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# series.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA[1].tail\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FB Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fbprophet.diagnostics import cross_validation\n",
    "\n",
    "dt = (DATA[1].copy())\n",
    "\n",
    "train_index = dt.index[:-12]\n",
    "test_index = dt.index[-12:]\n",
    "valid_index = dt.index[:-112]\n",
    "\n",
    "train_val = dt.values[:-12]\n",
    "test_val = dt.values[-12:]\n",
    "valid_val = dt.values[:-112]\n",
    "\n",
    "# office = office.rename(columns={'DatetimeIndex': 'ds', 'Sales': 'y'})\n",
    "\n",
    "dt_model = Prophet(daily_seasonality=True,\n",
    "                   seasonality_mode='additive')\n",
    "dt_model.add_seasonality(name='monthly', period=4, fourier_order=5)\n",
    "\n",
    "res_model = Prophet(daily_seasonality=True,\n",
    "                   seasonality_mode='additive')\n",
    "res_model.add_seasonality(name='monthly', period=4, fourier_order=5)\n",
    "\n",
    "validate = pd.DataFrame()\n",
    "validate[\"ds\"] = valid_index\n",
    "validate[\"y\"] = valid_val\n",
    "validate\n",
    "validate[\"floor\"] = validate['y'].min()\n",
    "validate[\"cap\"] = validate[\"y\"].max() + (validate['y'].max() * .2)\n",
    "\n",
    "dt_model.fit(validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "future = dt_model.make_future_dataframe(periods=12, include_history=True, freq=\"m\")\n",
    "future['floor'] = validate['y'].min()\n",
    "future['cap'] = validate['y'].max() + (validate['y'].max() *0.2)\n",
    "\n",
    "forecast = dt_model.predict(future)\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\n",
    "\n",
    "fig1 = dt_model.plot(forecast)\n",
    "DATA[1][:-100].plot(color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On the validation data (first 11 years) the model shows very good results. what about a full dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame()\n",
    "train[\"ds\"] = train_index\n",
    "train[\"y\"] = train_val\n",
    "train\n",
    "train[\"floor\"] = train['y'].min()\n",
    "train[\"cap\"] = train[\"y\"].max() + (train['y'].max() * .2)\n",
    "\n",
    "res_model.fit(train)\n",
    "\n",
    "future = res_model.make_future_dataframe(periods=12, include_history=True, freq=\"m\")\n",
    "future['floor'] = train['y'].min()\n",
    "future['cap'] = train['y'].max() + (train['y'].max() *0.2)\n",
    "\n",
    "forecast = dt_model.predict(future)\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\n",
    "\n",
    "fig1 = res_model.plot(forecast)\n",
    "DATA[1].plot(color='g')\n",
    "plt.grid()\n",
    "# plt.plot(time[:-12], train, 'b--', label=\"train\")\n",
    "plt.plot(test_index, test_val, color='orange', linestyle=\"--\", label=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not so good, but still beautiful. what about metrics?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_model.plot_components(forecast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cross_validation(model=res_model, horizon=\"365 days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_p = performance_metrics(res)\n",
    "df_p.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nothing better proposed approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.concatenate((DATA[0].values, DATA[1].values.reshape((-1, 1))), axis=1)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "values = scaler.fit_transform(values)\n",
    "\n",
    "VALIDATION_SHIFT = 12\n",
    "EPOCHS = 200\n",
    "\n",
    "train = values[:-VALIDATION_SHIFT, :]\n",
    "test = values[-VALIDATION_SHIFT:, :]\n",
    "\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "\n",
    "optimizer = Adagrad(0.01)\n",
    "\n",
    "# reshape input to be 3D [samples, features, timesteps]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(20, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "# model.compile(loss='mae', optimizer=optimizer)\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, \n",
    "                    epochs=EPOCHS, batch_size=64, \n",
    "                    validation_data=(test_X, test_y), \n",
    "                    verbose=2, shuffle=False)\n",
    "\n",
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "yhat[yhat<0] = 0\n",
    "yhat_train = model.predict(train_X)\n",
    "yhat_train[yhat_train<0] = 0\n",
    "\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "train_X = train_X.reshape((train_X.shape[0], train_X.shape[2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert scaling for training\n",
    "inv_yhat_train = np.concatenate((train_X, yhat_train), axis=1)\n",
    "inv_yhat_train = scaler.inverse_transform(inv_yhat_train)\n",
    "inv_yhat_train = inv_yhat_train[:,-1]\n",
    "\n",
    "\n",
    "# invert scaling for forecast\n",
    "inv_yhat = np.concatenate((test_X, yhat), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,-1]\n",
    "\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = np.concatenate((test_X, test_y), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,-1]\n",
    "\n",
    "\n",
    "# calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "print(\"Actual data: \", inv_y)\n",
    "print(\"Forecast: \",inv_yhat)\n",
    "\n",
    "plt.plot(pd.Series(inv_yhat_train, index=DATA[1].index[:-VALIDATION_SHIFT]), label=\"Trained forecast\")\n",
    "plt.plot(DATA[1], label=\"Actual data\")\n",
    "plt.plot(pd.Series(inv_yhat, index=DATA[1].index[-VALIDATION_SHIFT:]), label=\"Forecast\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], train_X.shape[1], 1))\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[1], 1))\n",
    "\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Conv1D(filters=64, kernel_size=3, activation='relu', \n",
    "                     input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model_cnn.add(MaxPooling1D(pool_size=2))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(50, activation='relu'))\n",
    "model_cnn.add(Dense(1))\n",
    "model_cnn.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "history = model_cnn.fit(train_X, train_y, \n",
    "                        epochs=EPOCHS, batch_size=64, \n",
    "                        validation_data=(test_X, test_y), \n",
    "                        verbose=2, shuffle=False)\n",
    "\n",
    "# plot history\n",
    "plt.plot(history.history['loss'][1:], label='train')\n",
    "plt.plot(history.history['val_loss'][1:], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# make a prediction\n",
    "yhat = model_cnn.predict(test_X)\n",
    "yhat[yhat<0] = 0\n",
    "yhat_train = model_cnn.predict(train_X)\n",
    "yhat_train[yhat_train<0] = 0\n",
    "\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[1]))\n",
    "train_X = train_X.reshape((train_X.shape[0], train_X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert scaling for training\n",
    "inv_yhat_train = np.concatenate((train_X, yhat_train), axis=1)\n",
    "inv_yhat_train = scaler.inverse_transform(inv_yhat_train)\n",
    "inv_yhat_train = inv_yhat_train[:,-1]\n",
    "\n",
    "\n",
    "# invert scaling for forecast\n",
    "inv_yhat = np.concatenate((test_X, yhat), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,-1]\n",
    "\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = np.concatenate((test_X, test_y), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,-1]\n",
    "\n",
    "\n",
    "# calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "print(\"Actual data: \", inv_y)\n",
    "print(\"Forecast: \",inv_yhat)\n",
    "\n",
    "plt.plot(pd.Series(inv_yhat_train, index=DATA[1].index[:-VALIDATION_SHIFT]), label=\"Trained forecast\")\n",
    "plt.plot(DATA[1], label=\"Actual data\")\n",
    "plt.plot(pd.Series(inv_yhat, index=DATA[1].index[-VALIDATION_SHIFT:]), label=\"Forecast\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion & prediction for one year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tested all models (except ML) and in my opinion, even ARIMA and fbprophet are very good approaches, and ARIMA fit's the model even better then fb, but for long term predictions it is better to use fb.\n",
    "\n",
    "My series is additive. \n",
    "\n",
    "About stationary series - the best way is to use EMA and sub from the data the series. \n",
    "\n",
    "Differencing is not very good approach in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet.diagnostics import cross_validation\n",
    "\n",
    "dt = (DATA[1].copy())[150:]\n",
    "\n",
    "res_model = Prophet(daily_seasonality=True,\n",
    "                   seasonality_mode='additive')\n",
    "res_model.add_seasonality(name='monthly', period=4, fourier_order=5)\n",
    "\n",
    "train = pd.DataFrame()\n",
    "train[\"ds\"] = dt.index\n",
    "train[\"y\"] = dt.values\n",
    "\n",
    "train[\"floor\"] = train['y'].min()\n",
    "train[\"cap\"] = train[\"y\"].max() + (train['y'].max() * .2)\n",
    "\n",
    "res_model.fit(train)\n",
    "\n",
    "future = res_model.make_future_dataframe(periods=12, include_history=True, freq=\"m\")\n",
    "future['floor'] = train['y'].min()\n",
    "future['cap'] = train['y'].max() + (train['y'].max() *0.2)\n",
    "\n",
    "forecast = dt_model.predict(future)\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\n",
    "\n",
    "fig1 = res_model.plot(forecast)\n",
    "dt.plot(color='g')\n",
    "plt.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
